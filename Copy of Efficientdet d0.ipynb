{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Efficientdet d0.ipynb","provenance":[{"file_id":"1RzUbSeDBjkTozrzLZRSR68DlqWjkhpx3","timestamp":1611249768753}],"authorship_tag":"ABX9TyMS5qD1MhGtldmPEg07zhyz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"E5NnutktzAgx"},"source":["Initailly mount the drive "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRmDUQ_izE1M","executionInfo":{"status":"ok","timestamp":1611225598430,"user_tz":480,"elapsed":941,"user":{"displayName":"Justin M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtKbK-W0cabQiLQmMCgVEOyuGuZ8onQgLdNr2HrqM=s64","userId":"02520081250919517143"}},"outputId":"451b304c-a37a-4b08-cf3b-a893cc3bb0c6"},"source":["from google.colab import drive\r\n","drive.mount(\"/content/gdrive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HB78eiJanEmE"},"source":["preprocessing the image data "]},{"cell_type":"code","metadata":{"id":"1o-iJw6OnH30"},"source":["from keras.preprocessing.image import ImageDataGenerator \r\n","!unzip '/content/gdrive/wobot/train'.zip -d '/content/gdrive/wobot/train_extracted'\r\n","!unzip '/content/gdrive/wobot/validation'.zip -d '/content/gdrive/wobot/validation_extracted'\r\n","train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\r\n","\r\n","#image input size for efficientdet is (224,224)\r\n","\r\n","training_set = train_datagen.flow_from_directory('/content/gdrive/wobot/train_extracted', target_size = (224,224), batch_size = 16, class_mode = 'categorical')\r\n","val_set = train_datagen.flow_from_directory('/content/gdrive/wobot/validation_extracted', target_size = (224,224), batch_size = 16, class_mode = 'categorical')\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TC7JOADV-u32"},"source":["Import and fine tune the pretrained efficientdet d0 model , remove last layer and add 62 output neurons since our data has 62 different classes\r\n"]},{"cell_type":"code","metadata":{"id":"zWcOalQeNodX"},"source":["import tensorflow as tf\r\n","#a pretrained model of efficientdet d0 is stored in google drive , after mounting the drive it is easy to import it here \r\n","from tensorflow.keras.models import load_model\r\n","#since we need to evaluate on different thresholds 2 different models are created \r\n","efiicientdet1=load_model(\"/content/gdrive/wobot/pretrained_efficientdet_d0.h5\",score_threshold=.5)\r\n","efiicientdet2=load_model(\"/content/gdrive/wobot/pretrained_efficientdet_d0.h5\",score_threshold=.75)\r\n","#now we need the whole layers of efficientdet model except the last layer since our custom data set contains 62 classes \r\n","new_model1=sequential()\r\n","for layer in efficientdet1.layers[:-1]:\r\n","  #adding every layer except the last layer\r\n","  new_model1.add(layer) \r\n","#since it is a multi  class problem softmax should be the activation function \r\n","new_model1.add(tf.keras.layers.Dense(units=62,activation='softmax'))\r\n","\r\n","\r\n","new_model2=sequential()\r\n","for layer in efficientdet2.layers[:-1]:\r\n","  new_model2.add(layer) #adding every layer except the last layer\r\n","#since it is a multi  class problem softmax should be the activation function \r\n","new_model2.add(tf.keras.layers.Dense(units=62,activation='softmax'))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i2shMGKh_E0x"},"source":["Train both models on the custom data set"]},{"cell_type":"code","metadata":{"id":"sS6EryrLODXV"},"source":["new_model1.complie(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\r\n","new_model1.fit(x=training_set,validation_data=val_set,epochs=25)\r\n","new_model2.complie(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\r\n","new_model2.fit(x=training_set,validation_data=val_set,epochs=25)\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RS1qs7yT_QZE"},"source":["Create lists for accuracy calculations"]},{"cell_type":"code","metadata":{"id":"kUeWuKc99_NQ"},"source":["y_true=[]\r\n","y_predicted_by_model1=[]\r\n","y_predicted_by_model2=[]\r\n","for i in range(61):\r\n","  y_true.append(i)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-vF-DLv_uuc"},"source":["Select random image from every class instance and calculate average precision "]},{"cell_type":"code","metadata":{"id":"oRam1z1C0IRl"},"source":["import numpy as np\r\n","import os,random\r\n","import numpy as np\r\n","from keras.preprocessing import image\r\n","\r\n","#preprocess the input image from the given path and convert it to network input format\r\n","def preprocessing_logic(path):\r\n","  img=image.load_img(path,target_size=(224,224))\r\n","  img=image.img_to_array(img)\r\n","  img=np.expand_dims(img,axis=0)\r\n","  return img\r\n","\r\n","#storing all class labels \r\n","class_labels=[]\r\n","for root, dirs, files in os.walk('/content/gdrive/wobot/images/validation/images'):\r\n","    for name in dirs:\r\n","        class_lables.append(name)\r\n","  \r\n","\r\n","#storing the root path name \r\n","path='/content/gdrive/wobot/images/validation/images'\r\n","\r\n","\r\n","\r\n","\r\n","for i in range(61):\r\n","    #the current class path\r\n","    path1=str(path+calss_labels[i])\r\n","    #selecting a random image forom the current class \r\n","    random_image_name=random.choice([x for x in os.listdir(path1)\r\n","               if os.path.isfile(os.path.join(path1, x))]) \r\n","    #path to image\r\n","    path_to_image=str(path1+random_image_name)  \r\n","    #pasing the image to the preprocessing function for processing \r\n","    preprocessed_image=preprocessing_logic(path_to_image)\r\n","    \r\n","    #making prediction using the first model\r\n","    result1=new_model1.predict(preprocessed_image) \r\n","    #making prediction using the second model        \r\n","    result2=new_model2.predict(preprocessed_image) \r\n","    #appending the class index predicted by the model1\r\n","    y_predicted_by_model1.append(result1[0][0])\r\n","    #appending the class index predicted by the model2\r\n","    y_predicted_by_model2.append(result2[0][0])\r\n","\r\n","\r\n","from sklearn.metrics import average_precision_score\r\n","#calculating the precision \r\n","mAP_model1=average_precision_score(y_true, y_predicted_by_model1)\r\n","mAP_model2=average_precision_score(y_true, y_predicted_by_model2)\r\n","print(mAP_model1)\r\n","print(mAP_model2)\r\n","   \r\n","\r\n","\r\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"orcaxIjAEDtT"},"source":["Writing results to CSV file"]},{"cell_type":"code","metadata":{"id":"3dz0MuPsEGK_"},"source":["import csv\r\n","with open('results.csv','w',newline='') as result_file\r\n","thewriter=csv.writer(result_file)\r\n","thewriter.writerow(['threshold .5', 'threshold .75'])\r\n","thewriter.writerow([mAP_model1,mAP_model2])"],"execution_count":null,"outputs":[]}]}